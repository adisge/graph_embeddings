{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLE (Locally Linear Embedding)\n",
    "\n",
    "LLE is one of the many dimensionality reduction methods based on spectral theory. LLE algorithm attempts to create a low dimensional embedding such that points that are nearby in high dimension remain close in low dimensional embedding. The error function is minimized in such a way that the neighborhood around a point does not changes in lower dimension. LLE uses conformal maps to solve the problem. Conformal or biholomorphic maps preserve the local angles between the points. It creates the embedding solely based on neighborhood distances without using global distances. LLE assumes that data lies on a smooth manifold(i.e it does not have holes) and each point and its neighbors lie approximately on a locally linear  patch on the manifold. The latter assumption gives us the freedom to express each point as a weighted sum of its neighbors.   \n",
    "\n",
    "LLE starts by building a conformal maps of the original dataset and then replicates it in the lower dimensions. \n",
    "Suppose , we have $Y_i$ where $i\\;\\in (1,...,n) \\;\\text{and}\\; Y_i\\;\\in R^D$. LLE begins by creating a neighborhood matrix. LLE assumes that dataset is large and well-sampled i.e. for every point we have enough to points to create a K-nearest or $\\epsilon$-ball neighborhood.   \n",
    "\n",
    "Once neighborhood matrix is defined, each point is reconstructed as a linear weighted sum of its neighbors. The cost fuction for reconstruction can be formulated as  \n",
    "$\\hspace{7em}\\mathcal{E}(W) = \\Sigma_{i}(Y_i - \\Sigma_{j\\in N(i)}^k\\; W_{ij}Y_j)^2$  \n",
    "LLE gets it's name from nature of these reconstructions. Since, only neighbors participate in reconstruction, it is *local* .Reconstruction is acheived by linear coefficients or weights, hence *linear*.  \n",
    "\n",
    "The weights $W_{ij} $ that contributed to the reconstruction of $X_i$  in higher dimesions should contribute to the reconstruction of the $X_i$ in lower dimension. Based on this idea,  a cost or error function is defined    \n",
    "$\\hspace{7em}\\Phi(X) = \\Sigma_{i}(X_i - \\Sigma_{j}W_{ij}X_j)^2$  where $X$ is the embedding we need to find. \n",
    "The difference between the two error functions defined above  is that the weights $W_{ij}$ are fixed in the latter case while in the former  they are variables. The embedding error does not depend on the Y(original dataset) and is a function of geometric information encoded by Weight matrix. With few suitable constraints, error function can be solved using eigenvalue decomposition of $(I-W)^T(I-W)$($I$ is an identity matrix) and a unique solution is obtained. The reconstruction weights of  points $Y_i$ and $Y_j$ are independent from each other. The eigenvalue decomposition is a global operation which process the information provided by all data points. This is the step where geometric information from the weight matrix is incorporated into the global structure of the embedding.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's go through each of these steps in details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Neighbor search   \n",
    "Neighbourhood can be created  through k-nearest neighbor or $\\epsilon$-ball neighborhood approach.  \n",
    "**K-nearest neighbor** - Each point is connected to its K-nearest points. Using this appraoch we will always have K-neighbors for each and every point. Since a point selects exactly K-points and it may be selected by some other point as neighbor which is not in his neighborhood set. This situation generally arises in case of an isolated point which selects faraway points as neighbors, while it's neighbors can select neighborhood set from smaller distance. This produces asymmetric neighborhood matrix.  \n",
    "**$\\epsilon$-ball neighbor** - Each point $Y_i$ selects every point inside the ball with radius $\\epsilon$ and centered at $Y_i$ as its neighbors. This approach sometimes leads to points with no neighbors. It is hard to find the right $\\epsilon$, since smaller value will give many isolated points and higher value will have many neighbors for each points. This approach is good for approximating geodesic distances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors \n",
    "from sklearn import datasets, neighbors\n",
    "import numpy as np\n",
    "# we will implement K-nearest neighbor search\n",
    "# le's load  fisher iris dataset\n",
    "iris_ = datasets.load_iris()\n",
    "\n",
    "iris = iris_.data\n",
    "def Knbor_Mat(X, K, t = 2.0, dist_metric = \"euclidean\", algorithm = \"ball_tree\"):\n",
    "    \n",
    "    n,p = X.shape\n",
    "    \n",
    "    knn = neighbors.NearestNeighbors(K+1, metric = dist_metric, algorithm=algorithm).fit(X)\n",
    "    distances, nbors = knn.kneighbors(X)\n",
    "    \n",
    "    return(nbors[:,1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculation of reconstruction weights\n",
    "It is also assumed that dataset is not too noisy, so that we don't have big outliers who distort the weights.  \n",
    "LLE tries to reconstruct the point $Y_i$ as the weighted average of its neighbors. The reconstruction error is given by  \n",
    "$\\hspace{7em}\\mathcal{E}(W) = \\Sigma_{i}(Y_i - \\Sigma_{j\\in N(i)}^k\\; W_{ij}Y_j)^2$ where,  \n",
    "$\\hspace{7em}N(i)$ is neighbourhood set of $Y_i$  and  \n",
    "$\\hspace{7em} \\Sigma_{j}W_{ij}\\; =\\; 1.0\\;$  \n",
    "The matrix W has an interesting property it is invariant to rescalings, rotations and translations. The invariance to rotations and rescalings comes from error formulation while $sum_{row}(W)$ = 1 keeps $W_{ij}$ unchanged in case of translations. Using matrix algebra, we get a closed from solution to this problem.   \n",
    "It is important to note that if the number of neighbors is greater than original dimension *D*, W will not have unique solution and some of the $W_{ij}$ might be zero. This issue can be handled  by adding a regularization term penalizing the large weights.  \n",
    "Steps to minimize $\\mathcal{E}(W)$  \n",
    "For $i$ in $1:n$;  \n",
    "$\\hspace{2em}$create a matrix $Z$ with all neighbors of $Y_i$  \n",
    "$\\hspace{2em}$subtract $Y_i$ from $Z$  \n",
    "$\\hspace{2em}$create the local covariance matrix $C = ZZ^T$  \n",
    "$\\hspace{2em}$Add a regularized term to avoid C being singular, $C$ $= C + reg*I$  \n",
    "$\\hspace{2em}$solve $CW = 1$ for $W$  \n",
    "$\\hspace{2em}$set $W_{ij} = 0$ if j is not a neighbor of i  \n",
    "$\\hspace{2em}$set $W = W/sum(W)$  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculation of reconstruction weights\n",
    "from scipy import linalg\n",
    "\n",
    "def get_weights(X, nbors, reg, K):\n",
    "    \n",
    "    n,p = X.shape\n",
    "    \n",
    "    Weights = np.zeros((n,n))\n",
    "    \n",
    "    for i in range(n):\n",
    "        \n",
    "        X_bors = X[nbors[i],:] - X[i]\n",
    "        cov_nbors = np.dot(X_bors, X_bors.T)\n",
    "        \n",
    "        #regularization tems\n",
    "        trace = np.trace(cov_nbors)\n",
    "        if trace >0 :\n",
    "            R = reg*trace\n",
    "        else:\n",
    "            R = reg\n",
    "        \n",
    "        cov_nbors.flat[::K+1] += R\n",
    "        weights = linalg.solve(cov_nbors, np.ones(K).T, sym_pos=True)\n",
    "\n",
    "        weights = weights/weights.sum()\n",
    "        Weights[i, nbors[i]] = weights\n",
    "        \n",
    "    return(Weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### calculate the embedded data using the weights\n",
    "Now, come's the last step of the algorithm i.e. computing the embedded data with the help of reconstruction weights. The error for reconstruction of data in lower dimension is given by  \n",
    "\n",
    "$\\hspace{7em}\\Phi(X) = \\Sigma_{i}(X_i - \\Sigma_{j}W_{ij}X_j)^2$  \n",
    "Since, $\\Sigma_{j}W_{ij} = 1.0$ we can write  \n",
    "$\\hspace{7em}\\Phi(X) = \\Sigma_{i}(\\Sigma_{j}W_{ij}(X_i - X_j)^2) = \\Sigma_{i}(\\Sigma_{j}(X_i - X_j)W_{ij}(X_i - X_j)^T) $   \n",
    "$\\hspace{9em}= \\text{tr}\\; X^TMX$    \n",
    "Now, matrix $M$ = $(I-W)^T(I-W)$.  \n",
    "We need to add few constraints to make sure we have stable solutions  \n",
    "$\\hspace{7em} \\Sigma_i X_i = 0$  \n",
    "$\\hspace{7em} XX^T = I\\;\\; I$ is identity matrix  \n",
    "Minimizing $\\Phi(X)$ with the constraints leads to unique solution in form of eigenvalue decomposition of M.\n",
    "We need to select $2^{nd}$ to $(p+1)^{th}$ smallest eigenvectors as the embedded data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculation of the new embedding\n",
    "from scipy.linalg import eigh\n",
    "\n",
    "def Y_(Weights,d):\n",
    "    n,p = Weights.shape\n",
    "    I = np.eye(n)\n",
    "    m = (I-Weights)\n",
    "    M = m.T.dot(m)\n",
    "    \n",
    "    eigvals, eigvecs = eigh(M, eigvals=(1, d), overwrite_a=True)\n",
    "    ind = np.argsort(np.abs(eigvals))\n",
    "    \n",
    "    return(eigvecs[:, ind])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "007270ba9615412b9afcc7c7e77a57f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=10, description='K', min=10, step=10), Output()), _dom_classes=('widget-…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plotter(K)>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 720x576 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import cm\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets\n",
    "\n",
    "n_points = 1000\n",
    "X, color = datasets.samples_generator.make_s_curve(n_points, random_state=0)\n",
    "\n",
    "def LLE_(X, K):\n",
    "    reg =0.001\n",
    "    nbors = Knbor_Mat(X,K)\n",
    "    print(nbors[0])\n",
    "    Weights = get_weights(X, nbors, reg,K)\n",
    "    \n",
    "    Y = Y_(Weights,2)\n",
    "    return(Y)\n",
    "test = [354,520, 246,134,3, 983, 186, 436, 893, 921]\n",
    "def plotter(K):\n",
    "    fig = plt.figure(figsize=(10,8))\n",
    "    Y = LLE_(X , K)\n",
    "    s = Y[test]\n",
    "    plt.scatter(Y[:,0],Y[:,1],c=color, cmap=plt.cm.spectral)\n",
    "    plt.scatter(s[:,0],s[:,1], c=\"black\")\n",
    "\n",
    "interact(plotter, K= widgets.IntSlider(min=10, max=100, value=10, step=10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLE in pyspark\n",
    "Let's change the LLE code above for pyspark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#necessary imports\n",
    "\n",
    "from sklearn import datasets\n",
    "from pyspark.sql import SQLContext as SQC\n",
    "from pyspark.mllib.linalg import Vectors as mllibVs, VectorUDT as mllibVUDT\n",
    "from pyspark.ml.linalg import Vectors as mlVs, VectorUDT as mlVUDT\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.mllib.linalg import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "from pyspark.mllib.linalg.distributed import IndexedRowMatrix\n",
    "import math as m\n",
    "import numpy as np\n",
    "\n",
    "sqc = SQC(sc)\n",
    "iris_tuple = datasets.load_iris()\n",
    "\n",
    "iris = iris_tuple.data\n",
    "\n",
    "df = (sqc.createDataFrame(sc.parallelize(iris.tolist()).zipWithIndex().\n",
    "                              map(lambda x: (x[1], mlVs.dense(x[0]))), [\"id\", \"features\"]))    \n",
    "\n",
    "\n",
    "\n",
    "udf_dist = udf(lambda x, y:  float(x.squared_distance(y)), DoubleType())\n",
    "\n",
    "\n",
    "df_2 = df\n",
    "    \n",
    "df = df.crossJoin(df ).toDF('x_id', 'x_feature', 'y_id', 'y_feature')\n",
    "df = df.withColumn(\"sim\", udf_dist(df.x_feature, df.y_feature))\n",
    "\n",
    "df = df.drop(\"x_feature\")\n",
    "\n",
    "st = struct([name for name in [\"y_id\", \"sim\",\"y_feature\"]]).alias(\"map\")\n",
    "df = df.select(\"x_id\", st)\n",
    "df  = df.groupby(\"x_id\").agg(collect_list(\"map\").alias(\"map\"))\n",
    "df = df.join(df_2, df_2.id == df.x_id, \"inner\").drop(\"x_id\")\n",
    "\n",
    "# calculate local neighborhood matrix\n",
    "\n",
    "def get_weights(map_list, k, features, reg):\n",
    "    \n",
    "\n",
    "    sorted_map = sorted(map_list, key = lambda x: x[1])[1:(k+1)]\n",
    "    \n",
    "    neighbors = np.array([s[2] for s in sorted_map])\n",
    "    ind = [s[0] for s in sorted_map]\n",
    "    \n",
    "    nbors, n_features = neighbors.shape\n",
    "    \n",
    "    neighbors_mat = neighbors - features.toArray().reshape(1,n_features)\n",
    "    \n",
    "    cov_neighbors = np.dot(neighbors_mat, neighbors_mat.T)\n",
    "    \n",
    "    # add regularization term\n",
    "    trace = np.trace(cov_neighbors)\n",
    "    if trace > 0:\n",
    "        R = reg * trace\n",
    "    else:\n",
    "        R = reg\n",
    "    \n",
    "    cov_neighbors.flat[::nbors + 1] += R\n",
    "    \n",
    "    weights = linalg.solve(cov_neighbors, np.ones(k).T, sym_pos=True)\n",
    "    weights = weights/weights.sum()\n",
    "    \n",
    "    full_weights = np.zeros(len(map_list))\n",
    "    full_weights[ind] = weights\n",
    "    \n",
    "    return(Vectors.dense(full_weights))\n",
    "\n",
    "udf_sort = udf(get_weights, mllibVUDT())\n",
    "\n",
    "df = df.withColumn(\"weights\", udf_sort(\"map\", lit(10), \"features\", lit(0.001)))\n",
    "\n",
    "# udf for creating a row of identity matrix \n",
    "def I(ind):\n",
    "    i = [0]*150\n",
    "    i[ind]=1.0\n",
    "    return(mllibVs.dense(i))\n",
    "\n",
    "# convert dataframe to indexedrowmatrix\n",
    "weights_irm = IndexedRowMatrix(df.select([\"id\",\"weights\"]).rdd.map(lambda x:(x[0],  I(x[0])-x[1])))\n",
    "M = weights_irm.toBlockMatrix().transpose().multiply( weights_irm.toBlockMatrix() )\n",
    "\n",
    "SVD = M.toIndexedRowMatrix().computeSVD(150, True)\n",
    "\n",
    "# select the vectors for low dimensional embedding\n",
    "lle_embbeding = np.fliplr(SVD.V.toArray()[:,-(d+1):-1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualization of **MNIST** dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advantages and Drawbacks\n",
    "LLE has got some nice properities. It is simple and provides an elegant closed form solution. The weight matrix is sparse, which makes computation easy. Like isomap, it is hard to find the neighborhood search parameter. Based on the slightly different $K$ or $\\epsilon$ LLE can produce completely different results.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\hspace{7em}\\Sigma_{i}(Y_i - \\Sigma_{j\\in N(i)}^k\\; W_{ij}Y_j)^2$ where,  \n",
    "$\\hspace{7em}N(i)$ is neighbourhood set of $Y_i$  and  \n",
    "$\\hspace{7em} \\Sigma_{j}W_{ij}\\; =\\; 1.0\\;$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
