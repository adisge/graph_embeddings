{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Locally Linear Embedding Method\n",
    "\n",
    "The LLE algorithm is one of the many dimensionality reduction methods based on spectral theory. It attempts to create a low dimensional embedding such that vectors that are nearby in high dimension remain close in low dimensional embedding. The error function is minimized in such a way that the neighborhood around a vector does not change in a lower dimension. The LLE  algorithm uses conformal maps to solve the problem. The conformal or biholomorphic maps preserve the local angles between the vectors. It creates embedding solely based on neighborhood distances without using global distances. LLE assumes that data lies on a smooth manifold (i.e., it does not have holes) and each vector and its neighbors lie approximately on a locally linear patch on the manifold. The latter assumption gives us the freedom to express each vector as a weighted sum of its neighbors.   \n",
    "\n",
    "LLE starts by building conformal maps of the original dataset and then replicates it in the lower dimensions. \n",
    "\n",
    "Suppose , we have $X_i$ where $i\\;\\in (1,...,n) \\;\\text{and}\\; X_i\\;\\in R^D$. LLE begins by creating a neighborhood matrix. It assumes that dataset is large and well-sampled, i.e. for every vector we have enough vectors to create a K-nearest or $\\epsilon$-ball neighborhood.   \n",
    "\n",
    "Once the neighborhood matrix is defined, each vector can be reconstructed as a linear weighted sum of its neighbors. The cost function for reconstruction can be formulated as:\n",
    " \n",
    "$\\hspace{7em}\\mathcal{E}(W) = \\Sigma_{i}(X_i - \\Sigma_{j\\in N(i)}^k\\; W_{ij}X_j)^2$  \n",
    "\n",
    "LLE obtained its name based on the nature of these reconstructions. Since only neighbors participate in reconstruction, it is *local*. The reconstruction has been achieved by linear coefficients or weights, hence *linear*.  \n",
    "\n",
    "The weights $W_{ij} $ that contributed to the reconstruction of $Y_i$  in higher dimensions should contribute to the reconstruction of the $Y_i$ in a lower dimension. Based on this idea,  the cost (error) function is defined as    \n",
    "\n",
    "$\\hspace{7em}\\Phi(Y) = \\Sigma_{i}(Y_i - \\Sigma_{j}W_{ij}Y_j)^2$  \n",
    "\n",
    "where $Y$ is the embedding, we need to find. \n",
    "\n",
    "The difference between the two error functions defined above is that the weights $W_{ij}$ are fixed in the latter case while in the former they are variables. The embedding error does not depend on the $X$ (original dataset) and is a function of geometric information encoded by the weight matrix. With few suitable constraints, error function can be solved using eigenvalue decomposition of $(I-W)^T(I-W)$ ($I$ is an identity matrix), and a unique solution was obtained. The reconstruction weights of vectors $X_i$ and $X_j$ are independent of each other. The eigenvalue decomposition is a global operation which processes the information provided by all data. It presents the step where geometric information from the weight matrix has incorporated into the global structure of the embedding.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Neighbor search   \n",
    "\n",
    "Neighbourhood can be created  through k-nearest neighbor (k-NN) or $\\epsilon$-ball neighborhood approach.  \n",
    "\n",
    "**K-nearest neighbor** - At this method, each vector was connected to its K-nearest vectors. By using this technique, we will always have K-neighbors for every vector. Since a vector selects K-vectors and some other vector may select it as a neighbor who is not in his neighborhood set. This situation generally arises in case of an isolated vector which selects faraway vectors as neighbors, while its neighbors can select neighborhood set from a smaller distance. It produces asymmetric neighborhood matrix.  \n",
    "\n",
    "\n",
    "**$\\epsilon$-ball neighbor** - Each vector $X_i$ selects every vector inside the ball with radius $\\epsilon$ and centered at $X_i$ as its neighbors. This approach sometimes leads to vectors with no neighbors. It is hard to find the right $\\epsilon$ since the smaller value will give many isolated vectors and higher value will have many neighbors for each vector. This approach is useful for approximating geodesic distances.\n",
    "\n",
    "**Step 1: The seeking of neighbors in Python**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors \n",
    "from sklearn import datasets, neighbors\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# The implementation of K-nearest neighbor (K-NN) search method\n",
    "\n",
    "\n",
    "def KNNAlgorithm(X, K, t = 2.0, dist_metric = \"euclidean\", algorithm = \"ball_tree\"):\n",
    "    \n",
    "    #n, p = X.shape\n",
    "    \n",
    "    knn = neighbors.NearestNeighbors(K+1, metric = dist_metric, algorithm=algorithm).fit(X)\n",
    "    \n",
    "    distances, nbors = knn.kneighbors(X)\n",
    "    \n",
    "    return(nbors[:,1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The computing of the reconstruction weights W\n",
    "\n",
    "We will also assume that dataset is not too noisy so that we do not have significant outliers who distort the weights. \n",
    "\n",
    "LLE tries to reconstruct the vector $X_i$ as the weighted average of its neighbors. The reconstruction error is given by  \n",
    "\n",
    "$\\hspace{7em}\\mathcal{E}(W) = \\Sigma_{i}(X_i - \\Sigma_{j\\in N(i)}^k\\; W_{ij}X_j)^2$ \n",
    "\n",
    "where \n",
    "\n",
    "$\\hspace{7em}N(i)$ is neighbourhood set of $X_i$ and \n",
    "\n",
    "$\\hspace{7em} \\Sigma_{j}W_{ij}\\; =\\; 1.0\\;$  \n",
    "\n",
    "\n",
    "The matrix $W$ has a property that it is invariant to rescalings, rotations, and translations. The invariance to rotations and rescalings comes from error formulation while $sum_{row}(W)$ = 1 keeps $W_{ij}$ unchanged in case of translations. Using matrix algebra, we get a closed form solution to this problem. \n",
    "\n",
    "It is important to note that if the number of neighbors K is higher than original dimension *D*, we will not have the unique solution and some of the $W_{ij}$ might be zero. This issue can be handled by adding a regularization term penalizing the large weights.  \n",
    "\n",
    "Steps to minimize $\\mathcal{E}(W)$  \n",
    "\n",
    "For $i$ in $1:n$;  \n",
    "$\\hspace{2em}$create a matrix $Z$ with all neighbors of $X_i$  \n",
    "$\\hspace{2em}$subtract $X_i$ from $Z$  \n",
    "$\\hspace{2em}$create the local covariance matrix $C = ZZ^T$  \n",
    "$\\hspace{2em}$Add a regularized term to avoid C being singular, $C$ $= C + reg*I$  \n",
    "$\\hspace{2em}$solve $CW = 1$ for $W$  \n",
    "$\\hspace{2em}$set $W_{ij} = 0$ if j is not a neighbor of i  \n",
    "$\\hspace{2em}$set $W = W/sum(W)$  \n",
    "\n",
    "\n",
    "\n",
    "**Step 2: The computing of the reconstruction weights W in Python**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The calculation of the reconstruction weights W\n",
    "\n",
    "from scipy import linalg\n",
    "\n",
    "def getReconstructionWeights(X, nbors, reg, K):\n",
    "    \n",
    "    N, D = X.shape\n",
    "    \n",
    "    Weights = np.zeros((N, N))\n",
    "    \n",
    "    for i in range(N):\n",
    "        \n",
    "        X_bors = X[nbors[i],:] - X[i] # subtract $X_i$ from $Z$, where Z=nbors[i] for all i  \n",
    "        \n",
    "        cov_nbors = np.dot(X_bors, X_bors.T) # The local covariance matrix $C = ZZ^T$ \n",
    "        \n",
    "        \n",
    "        trace = np.trace(cov_nbors) # The regularization term\n",
    "        if trace >0 :\n",
    "            R = reg*trace \n",
    "        else:\n",
    "            R = reg\n",
    "        \n",
    "        # R is a regularized term to avoid C being singular\n",
    "        \n",
    "        cov_nbors.flat[::K+1] += R #C+=reg*I\n",
    "        \n",
    "        weights = linalg.solve(cov_nbors, np.ones(K).T, sym_pos=True) # Solve $CW = 1$ for $W \n",
    "\n",
    "        weights = weights/weights.sum()\n",
    "        \n",
    "        Weights[i, nbors[i]] = weights \n",
    "        \n",
    "    return(Weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The calculation of the embedded data using the weights W\n",
    "\n",
    "Now, come's the last step of the algorithm, i.e. computing the embedded data with the help of reconstruction weights. The error for the reconstruction of data in a lower dimension is given by  \n",
    "\n",
    "$\\hspace{7em}\\Phi(Y) = \\Sigma_{i}(Y_i - \\Sigma_{j}W_{ij}Y_j)^2$  \n",
    "\n",
    "\n",
    "Since, $\\Sigma_{j}W_{ij} = 1.0$ we can write  \n",
    "\n",
    "$\\hspace{7em}\\Phi(Y) = \\Sigma_{i}(\\Sigma_{j}W_{ij}(Y_i - Y_j)^2) = \\Sigma_{i}(\\Sigma_{j}(Y_i - Y_j)W_{ij}(Y_i - Y_j)^T) $   \n",
    "$\\hspace{9em}= \\text{tr}\\; Y^TMX$    \n",
    "\n",
    "whre the matrix $M$ is given by $M$ = $(I-W)^T(I-W)$. We need to add few constraints to make sure we have stable solutions  \n",
    "\n",
    "$\\hspace{7em} \\Sigma_i Y_i = 0$  \n",
    "$\\hspace{7em} YY^T = I\\;\\; and I$ is identity matrix  \n",
    "\n",
    "\n",
    "Minimizing $\\Phi(Y)$ with the constraints leads to unique solution in form of eigenvalue decomposition of $M$.\n",
    "\n",
    "We need to select $2^{Nd}$ to $(K+1)^{th}$ smallest eigenvectors as the embedded data.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**Step 3: The computing of the embedded vectors by using the weights W in Python**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The computing the bottom eigenvectors (embedding coordinates) by using weights W\n",
    "\n",
    "from scipy.linalg import eigh\n",
    "\n",
    "def getEmbeddedVectors(Weights, d):\n",
    "    \n",
    "    N, K = Weights.shape\n",
    "    \n",
    "    I = np.eye(N)\n",
    "    \n",
    "    m = (I-Weights)\n",
    "    \n",
    "    M = m.T.dot(m)\n",
    "    \n",
    "    eigvals, eigvecs = eigh(M, eigvals=(1, d), overwrite_a=True)\n",
    "    \n",
    "    ind = np.argsort(np.abs(eigvals))\n",
    "    \n",
    "    return(eigvecs[:, ind])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The LLE Algorithm \n",
    "\n",
    "def lleAlgorithm(X, K, d, reg):\n",
    "       \n",
    "    nbors= KNNAlgorithm(X, K)\n",
    "    \n",
    "    Weights = getReconstructionWeights(X, nbors, reg, K)\n",
    "    \n",
    "    Y = getEmbeddedVectors(Weights, d)\n",
    "    \n",
    "    return(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b9fc042d0b04b98bd3866bbd7694ec4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=10, description='K', min=10, step=10), Output()), _dom_classes=('widget-…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.draw(K)>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The Main Program\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import cm\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "#import ipywidgets as widgets\n",
    "\n",
    "reg=0.001\n",
    "d=2\n",
    "K=10\n",
    "n_points = 1000\n",
    "X, color = datasets.samples_generator.make_s_curve(n_points, random_state=0)\n",
    "\n",
    "N, D= X.shape # D is equal 3, and N=1000\n",
    "\n",
    "Y = lleAlgorithm(X, K, d, reg)\n",
    "\n",
    "test = [354, 520, 246, 134, 3, 983, 186, 436, 893, 921]\n",
    "s = Y[test]\n",
    "\n",
    "#fig = plt.figure(figsize=(10,8))\n",
    "#plt.scatter(Y[:,0],Y[:,1],c=color, cmap = cm.get_cmap(\"Spectral\"))\n",
    "#plt.scatter(s[:,0],s[:,1], c=\"black\")\n",
    "\n",
    "def draw(K):\n",
    "    fig = plt.figure(figsize=(10,8))\n",
    "    reg=0.0001\n",
    "    Y = lleAlgorithm(X, K, 2, reg)\n",
    "    s = Y[test]\n",
    "    plt.scatter(Y[:,0],Y[:,1],c=color, cmap = cm.get_cmap(\"Spectral\"))\n",
    "    plt.scatter(s[:,0],s[:,1], c=\"black\")\n",
    "    \n",
    "interact(draw, K= widgets.IntSlider(min=10, max=100, value=10, step=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
