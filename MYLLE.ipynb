{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Locally Linear Embedding Method\n",
    "\n",
    "LLE algorithm is one of the many dimensionality reduction methods based on spectral theory. LLE algorithm attempts to create a low dimensional embedding such that points that are nearby in high dimension remain close in low dimensional embedding. The error function is minimized in such a way that the neighborhood around a point does not changes in lower dimension. LLE uses conformal maps to solve the problem. Conformal or biholomorphic maps preserve the local angles between the points. It creates the embedding solely based on neighborhood distances without using global distances. LLE assumes that data lies on a smooth manifold(i.e it does not have holes) and each point and its neighbors lie approximately on a locally linear  patch on the manifold. The latter assumption gives us the freedom to express each point as a weighted sum of its neighbors.   \n",
    "\n",
    "LLE starts by building a conformal maps of the original dataset and then replicates it in the lower dimensions. \n",
    "Suppose , we have $Y_i$ where $i\\;\\in (1,...,n) \\;\\text{and}\\; Y_i\\;\\in R^D$. LLE begins by creating a neighborhood matrix. LLE assumes that dataset is large and well-sampled i.e. for every point we have enough to points to create a K-nearest or $\\epsilon$-ball neighborhood.   \n",
    "\n",
    "Once neighborhood matrix is defined, each point is reconstructed as a linear weighted sum of its neighbors. The cost fuction for reconstruction can be formulated as  \n",
    "$\\hspace{7em}\\mathcal{E}(W) = \\Sigma_{i}(Y_i - \\Sigma_{j\\in N(i)}^k\\; W_{ij}Y_j)^2$  \n",
    "LLE gets it's name from nature of these reconstructions. Since, only neighbors participate in reconstruction, it is *local* .Reconstruction is acheived by linear coefficients or weights, hence *linear*.  \n",
    "\n",
    "The weights $W_{ij} $ that contributed to the reconstruction of $X_i$  in higher dimesions should contribute to the reconstruction of the $X_i$ in lower dimension. Based on this idea,  a cost or error function is defined    \n",
    "$\\hspace{7em}\\Phi(X) = \\Sigma_{i}(X_i - \\Sigma_{j}W_{ij}X_j)^2$  where $X$ is the embedding we need to find. \n",
    "The difference between the two error functions defined above  is that the weights $W_{ij}$ are fixed in the latter case while in the former  they are variables. The embedding error does not depend on the Y(original dataset) and is a function of geometric information encoded by Weight matrix. With few suitable constraints, error function can be solved using eigenvalue decomposition of $(I-W)^T(I-W)$($I$ is an identity matrix) and a unique solution is obtained. The reconstruction weights of  points $Y_i$ and $Y_j$ are independent from each other. The eigenvalue decomposition is a global operation which process the information provided by all data points. This is the step where geometric information from the weight matrix is incorporated into the global structure of the embedding.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
